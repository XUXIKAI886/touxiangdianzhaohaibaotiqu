# 美团外卖店铺图片提取教程

## 概述

本文档详细说明如何从美团外卖API返回的数据中提取店铺的头像、店招和海报图片,并下载原始高清图片。

## 数据来源

美团外卖的店铺数据通常来自以下API接口:
```
POST https://wx.waimai.meituan.com/weapp/v1/poi/food
```

返回的数据是JSON格式,包含店铺的完整信息。

### 使用Fiddler抓取数据

要获取店铺的原始数据,可以使用**Fiddler**抓包工具来拦截和保存API请求的响应数据。

#### 准备工作

1. **下载安装Fiddler**
   - 官网: https://www.telerik.com/fiddler
   - 支持Windows平台
   - 推荐使用Fiddler Classic或Fiddler Everywhere

2. **配置Fiddler**
   - 启动Fiddler
   - 菜单栏: `Tools` → `Options` → `HTTPS`
   - 勾选 `Capture HTTPS CONNECTs`
   - 勾选 `Decrypt HTTPS traffic`
   - 点击 `Actions` → `Trust Root Certificate` (信任根证书)
   - 重启Fiddler使配置生效

#### 抓包步骤

**步骤1: 启动Fiddler监听**

1. 打开Fiddler,确保左下角显示 `Capturing` (正在捕获)
2. 如果显示 `Not Capturing`,按 `F12` 键开始捕获

**步骤2: 设置过滤器 (可选但推荐)**

为了更容易找到目标请求,可以设置过滤条件:

1. 右侧面板点击 `Filters` 标签
2. 勾选 `Use Filters`
3. 在 `Hosts` 区域:
   - 选择 `Show only the following Hosts`
   - 输入: `wx.waimai.meituan.com`
4. 点击 `Actions` → `Run Filterset now`

**步骤3: 打开美团外卖小程序/网页**

1. 打开微信小程序 "美团外卖" 或访问美团外卖网页版
2. 浏览进入某个店铺的详情页面
3. 等待页面完全加载

**步骤4: 查找目标请求**

在Fiddler左侧会话列表中:

1. 查找URL包含 `wx.waimai.meituan.com/weapp/v1/poi/food` 的请求
2. 该请求通常是 `POST` 方法
3. 点击选中该会话

**步骤5: 查看响应数据**

在右侧面板:

1. 点击 `Inspectors` 标签
2. 选择 `Raw` 或 `JSON` 视图
3. 可以看到完整的HTTP响应内容

**步骤6: 保存数据**

方法1: 直接保存响应体
1. 右键点击目标会话
2. 选择 `Save` → `Response` → `Response Body`
3. 保存为 `.txt` 或 `.json` 文件

方法2: 保存完整会话
1. 右键点击目标会话
2. 选择 `Save` → `Selected Sessions` → `as Text`
3. 保存为 `.txt` 文件 (包含请求头和响应头)

方法3: 复制到剪贴板
1. 在右侧 `Inspectors` → `JSON` 视图
2. 全选响应内容 (Ctrl+A)
3. 复制 (Ctrl+C)
4. 粘贴到文本编辑器并保存

#### Fiddler界面说明

```
┌─────────────────────────────────────────────────────────┐
│  Fiddler Classic                              [_][□][×] │
├─────────────────────────────────────────────────────────┤
│  File  Edit  Rules  Tools  View  Help                  │
├──────────────┬──────────────────────────────────────────┤
│              │  Inspectors                              │
│  会话列表      │  ┌────────────────────────────────────┐ │
│              │  │ Headers  TextView  JSON  Raw  ... │ │
│  #  Result   │  ├────────────────────────────────────┤ │
│  50  200     │  │                                    │ │
│  51  200     │  │  {                                 │ │
│  52  200 ◄──┼──┤    "msg": "成功",                  │ │
│  53  304     │  │    "code": 0,                      │ │
│  ...         │  │    "data": {                       │ │
│              │  │      "poi_info": {                 │ │
│              │  │        "pic_url": "http://...",    │ │
│              │  │        ...                         │ │
│              │  │      }                             │ │
│              │  │    }                               │ │
│              │  │  }                                 │ │
│              │  │                                    │ │
│              │  └────────────────────────────────────┘ │
└──────────────┴──────────────────────────────────────────┘
   Capturing                              All Processes
```

#### 识别正确的请求

正确的 `poi/food` 请求特征:

- **URL**: `https://wx.waimai.meituan.com/weapp/v1/poi/food?...`
- **方法**: `POST`
- **状态码**: `200`
- **Content-Type**: `application/json`
- **响应大小**: 通常 15-30 KB
- **响应内容**: 包含店铺名称、图片URL、菜单等信息

#### 批量抓取技巧

如果需要抓取多个店铺的数据:

1. **方法1: 手动逐个浏览**
   - 在小程序/网页中逐个进入店铺
   - Fiddler会自动记录所有请求
   - 最后批量保存所有 `poi/food` 会话

2. **方法2: 使用自动化脚本**
   - Fiddler支持FiddlerScript (基于JScript.NET)
   - 可以编写脚本自动保存特定URL的响应

3. **方法3: 导出所有会话**
   - `File` → `Export Sessions` → `All Sessions`
   - 选择格式: `HTTPArchive (HAR)` 或 `SAZ (Fiddler Archive)`
   - 后续用程序批量解析

#### 自动保存脚本示例

在Fiddler中启用自动保存脚本:

1. 菜单 `Rules` → `Customize Rules...` (会打开FiddlerScript编辑器)
2. 找到 `OnBeforeResponse` 函数
3. 添加以下代码:

```javascript
static function OnBeforeResponse(oSession: Session) {
    // 自动保存 poi/food 接口的响应(每次覆盖同一文件)
    if (oSession.uriContains("/weapp/v1/poi/food")) {
        // 先解压缩响应数据(如果是gzip或deflate压缩的)
        oSession.utilDecodeResponse();

        // 固定文件名,每次覆盖
        var filename = "E:\\美团外卖数据\\latest_poi_food.json";

        // 获取响应体文本
        var responseText = oSession.GetResponseBodyAsString();

        // 保存为UTF-8编码的文本文件(覆盖模式)
        var fs = System.IO.File.Create(filename);
        var bytes = System.Text.Encoding.UTF8.GetBytes(responseText);
        fs.Write(bytes, 0, bytes.Length);
        fs.Close();

        // 在Fiddler中显示提示
        FiddlerObject.StatusText = "Saved (overwrite): " + filename;
    }
}
```

4. 保存脚本 (Ctrl+S)
5. 确保目录 `E:\美团外卖数据\` 存在
6. 之后每次捕获到 `poi/food` 请求,都会**覆盖**同一个文件 `latest_poi_food.json`

**重要说明**:
- 使用固定文件名 `latest_poi_food.json`,每次抓包直接覆盖
- `oSession.utilDecodeResponse()` 会自动解压gzip/deflate等压缩格式
- 使用 `GetResponseBodyAsString()` 获取解压后的文本内容
- 明确指定UTF-8编码保存,避免中文乱码
- 适合配合自动提取脚本使用,始终处理最新的店铺数据

#### 移动设备抓包

如果需要抓取手机微信小程序的数据:

**准备工作**
1. 电脑和手机连接到同一WiFi
2. 在Fiddler中: `Tools` → `Options` → `Connections`
3. 勾选 `Allow remote computers to connect`
4. 记下 `Fiddler listens on port` (默认8888)
5. 重启Fiddler

**手机配置**
1. 打开手机WiFi设置
2. 点击已连接的WiFi网络,进入详细设置
3. 配置代理:
   - 代理模式: 手动
   - 服务器: 电脑的IP地址 (如 192.168.1.100)
   - 端口: 8888 (Fiddler的端口)
4. 保存设置

**安装证书** (仅首次需要)
1. 手机浏览器访问: `http://电脑IP:8888`
2. 点击 `FiddlerRoot certificate` 下载证书
3. 安装证书 (具体步骤因手机系统而异)

**开始抓包**
1. 在手机上打开微信小程序 "美团外卖"
2. 浏览店铺
3. 在电脑Fiddler中查看捕获的请求

#### 注意事项

⚠️ **安全提醒**
- Fiddler会解密HTTPS流量,可能被安全软件警告
- 仅用于学习和个人研究目的
- 不要在生产环境或公共WiFi使用
- 完成抓包后及时关闭代理设置

⚠️ **数据隐私**
- 抓取的数据可能包含个人信息(地址、用户ID等)
- 妥善保管数据文件,不要泄露或分享
- 仅用于个人学习,不要用于商业用途

⚠️ **请求频率**
- 不要短时间内频繁请求,可能触发反爬机制
- 建议每个请求间隔3-5秒
- 避免使用自动化工具大量抓取

## 图片类型说明

### 1. 店铺头像 (Avatar)
- **字段名**: `pic_url`
- **位置**: `data.poi_info.pic_url`
- **用途**: 店铺的Logo/头像,通常显示在店铺列表、详情页等位置
- **特点**: 正方形图片

### 2. 店铺店招 (Header Banner)
- **字段名**: `head_pic_url`
- **位置**: `data.poi_info.head_pic_url`
- **用途**: 店铺详情页顶部的横幅图片
- **特点**: 宽屏横向图片,展示店铺环境或特色

### 3. 店铺海报 (Promotional Poster)
- **字段名**: `pic_url` (在活动专场中)
- **位置**: `data.container_operation_source.operation_source_list[0].pic_url`
- **用途**: 活动宣传海报,展示促销活动或特色商品
- **特点**: 包含活动信息的宣传图

## 自动化工作流程 (推荐)

配合Fiddler自动保存脚本,实现完全自动化的图片提取流程:

### 工作流程

```
1. Fiddler监听 → 2. 抓取数据 → 3. 自动覆盖到 latest_poi_food.json
                                    ↓
4. 运行提取脚本 ← 3. 自动下载图片 ← 2. 解析JSON提取URL
```

### 步骤1: 配置Fiddler自动保存

参考前面的"自动保存脚本示例",使用覆盖模式的脚本,每次抓包自动保存到固定文件 `latest_poi_food.json`。

### 步骤2: 运行自动提取脚本

使用提供的Python脚本自动提取并下载图片:

```bash
python 自动提取图片.py
```

脚本功能:
- 自动读取 `latest_poi_food.json`
- 提取店铺名称、ID和三张图片URL
- 自动移除尺寸参数,下载原图
- 按店铺名称+时间戳创建文件夹
- 保存图片和URL信息

### 完整使用流程

```bash
# 1. 启动Fiddler,加载自动保存脚本
#    (只需设置一次)

# 2. 在微信小程序浏览店铺
#    Fiddler会自动抓取并保存到 latest_poi_food.json

# 3. 运行提取脚本
python 自动提取图片.py

# 4. 查看结果
#    图片保存在: E:\美团外卖数据\店铺图片\店铺名称_时间戳\
```

### 输出结构

```
店铺图片/
└── 益禾堂（宜昌山水华庭店）_20251011_214530/
    ├── 店铺头像.jpg        (260 KB - 原图)
    ├── 店铺店招.jpg        (1.2 MB - 原图)
    ├── 店铺海报.jpg        (215 KB)
    └── urls.txt           (图片URL记录)
```

---

## 手动提取步骤

如果不使用自动化脚本,也可以手动提取:

### 步骤1: 读取数据文件

假设你有一个包含API响应的文本文件 `52_Full.txt`:

```bash
# 查看文件内容
cat 52_Full.txt
```

### 步骤2: 解析JSON数据

如果数据是纯JSON格式,可以使用工具提取。在本例中,数据从第42行开始是JSON内容。

### 步骤3: 定位图片URL

在JSON数据中搜索关键字段:

#### 3.1 提取店铺头像URL

搜索路径: `data.poi_info.pic_url`

示例:
```json
"pic_url":"http://p0.meituan.net/waimaipoi/53336df361c469e55a236e83296f36b9425033.jpg@130w_130h_1e_1c"
```

#### 3.2 提取店铺店招URL

搜索路径: `data.poi_info.head_pic_url`

示例:
```json
"head_pic_url":"http://p1.meituan.net/business/6b1d48ff742d8c81c2e25d0683836ed4108601.jpg?t=1760088771653@750w_310h_1e_1c"
```

#### 3.3 提取店铺海报URL

搜索路径: `data.container_operation_source.operation_source_list[0].pic_url`

示例:
```json
"pic_url":"http://p0.meituan.net/business/3df1b39e3475a98ab98f90cb168c6b51357677.jpg?t=1760086957439"
```

### 步骤4: 处理URL参数

**重要**: 美团图片URL后面的参数会控制图片尺寸,需要去除才能获取原图。

#### URL参数说明

- `@130w_130h_1e_1c` - 压缩为130x130像素
- `@750w_310h_1e_1c` - 压缩为750x310像素
- `?t=时间戳` - 缓存参数,可保留或删除

#### 参数处理规则

1. **移除 @ 后面的所有内容** (包括@符号)
2. **时间戳参数可选** - `?t=xxx` 可以保留也可以删除

#### 处理示例

原始URL:
```
http://p0.meituan.net/waimaipoi/53336df361c469e55a236e83296f36b9425033.jpg@130w_130h_1e_1c
```

处理后 (原图URL):
```
http://p0.meituan.net/waimaipoi/53336df361c469e55a236e83296f36b9425033.jpg
```

### 步骤5: 下载图片

使用 `curl` 命令下载图片:

#### 下载店铺头像
```bash
curl -o "店铺头像_原图.jpg" "http://p0.meituan.net/waimaipoi/53336df361c469e55a236e83296f36b9425033.jpg"
```

#### 下载店铺店招
```bash
curl -o "店铺店招_原图.jpg" "http://p1.meituan.net/business/6b1d48ff742d8c81c2e25d0683836ed4108601.jpg"
```

#### 下载店铺海报
```bash
curl -o "店铺海报.jpg" "http://p0.meituan.net/business/3df1b39e3475a98ab98f90cb168c6b51357677.jpg"
```

## 完整示例

### 示例数据结构

```json
{
  "data": {
    "poi_info": {
      "name": "四喜烤蹄（烧烤·伍家岗店）",
      "pic_url": "http://p0.meituan.net/waimaipoi/53336df361c469e55a236e83296f36b9425033.jpg@130w_130h_1e_1c",
      "head_pic_url": "http://p1.meituan.net/business/6b1d48ff742d8c81c2e25d0683836ed4108601.jpg?t=1760088771653@750w_310h_1e_1c",
      "poi_back_pic_url": "http://p0.meituan.net/aichequan/0db65652de81b2140a9eb6c2183f3296361823.png"
    },
    "container_operation_source": {
      "operation_source_list": [
        {
          "pic_url": "http://p0.meituan.net/business/3df1b39e3475a98ab98f90cb168c6b51357677.jpg?t=1760086957439"
        }
      ]
    }
  }
}
```

### 提取脚本示例 (Bash)

```bash
#!/bin/bash

# 定义输出目录
OUTPUT_DIR="./images"
mkdir -p "$OUTPUT_DIR"

# 店铺头像URL (去除尺寸参数)
AVATAR_URL="http://p0.meituan.net/waimaipoi/53336df361c469e55a236e83296f36b9425033.jpg"

# 店铺店招URL (去除尺寸参数)
HEADER_URL="http://p1.meituan.net/business/6b1d48ff742d8c81c2e25d0683836ed4108601.jpg"

# 店铺海报URL
POSTER_URL="http://p0.meituan.net/business/3df1b39e3475a98ab98f90cb168c6b51357677.jpg"

# 下载图片
echo "开始下载店铺图片..."

curl -o "$OUTPUT_DIR/店铺头像.jpg" "$AVATAR_URL"
echo "✓ 店铺头像下载完成"

curl -o "$OUTPUT_DIR/店铺店招.jpg" "$HEADER_URL"
echo "✓ 店铺店招下载完成"

curl -o "$OUTPUT_DIR/店铺海报.jpg" "$POSTER_URL"
echo "✓ 店铺海报下载完成"

echo "所有图片下载完成！"
```

### Python脚本示例

```python
import json
import requests
import re

def remove_size_params(url):
    """移除美团图片URL中的尺寸参数"""
    # 移除 @ 后面的所有内容
    url = re.sub(r'@\d+w_\d+h_\d+e_\d+c', '', url)
    return url

def extract_and_download_images(json_file, output_dir='./images'):
    """从JSON文件中提取并下载店铺图片"""
    import os
    os.makedirs(output_dir, exist_ok=True)

    # 读取JSON文件
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    poi_info = data['data']['poi_info']

    # 1. 提取并处理店铺头像URL
    avatar_url = remove_size_params(poi_info['pic_url'])
    print(f"店铺头像URL: {avatar_url}")

    # 2. 提取并处理店铺店招URL
    header_url = remove_size_params(poi_info['head_pic_url'])
    print(f"店铺店招URL: {header_url}")

    # 3. 提取店铺海报URL
    operation_list = data['data']['container_operation_source']['operation_source_list']
    if operation_list:
        poster_url = operation_list[0]['pic_url']
        # 移除时间戳参数（可选）
        poster_url = re.sub(r'\?t=\d+', '', poster_url)
        print(f"店铺海报URL: {poster_url}")

    # 下载图片
    images = {
        '店铺头像.jpg': avatar_url,
        '店铺店招.jpg': header_url,
        '店铺海报.jpg': poster_url
    }

    for filename, url in images.items():
        filepath = os.path.join(output_dir, filename)
        print(f"正在下载: {filename}...")

        response = requests.get(url)
        if response.status_code == 200:
            with open(filepath, 'wb') as f:
                f.write(response.content)
            print(f"✓ {filename} 下载完成 ({len(response.content)} 字节)")
        else:
            print(f"✗ {filename} 下载失败: HTTP {response.status_code}")

    print("\n所有图片下载完成！")

# 使用示例
if __name__ == '__main__':
    extract_and_download_images('52_Full.json', './店铺图片')
```

## 注意事项

### 1. 图片尺寸问题
- **务必移除URL中的尺寸参数** (`@130w_130h_1e_1c` 等),否则下载的是压缩后的小图
- 原图通常比压缩图大几十倍甚至上百倍

### 2. 文件大小对比

| 图片类型 | 压缩图大小 | 原图大小 | 放大倍数 |
|---------|-----------|----------|---------|
| 店铺头像 | 8.4 KB    | 260 KB   | 31倍    |
| 店铺店招 | 67.9 KB   | 67.9 KB  | 相同    |
| 店铺海报 | -         | 215 KB   | -       |

### 3. URL参数说明

美团图片URL参数格式:
```
@{width}w_{height}h_{quality}e_{mode}c
```

参数含义:
- `width`: 宽度(像素)
- `height`: 高度(像素)
- `quality`: 质量等级
- `mode`: 裁剪模式

### 4. 其他字段

除了主要的三张图片,数据中还可能包含:

- `poi_back_pic_url`: 页面背景图(通常是渐变或纯色)
- `promotion_head_pic_url`: 促销头图(可能为空)
- `poi_logo_icon`: 品牌Logo图标
- `poi_tab_icon`: 标签图标

### 5. 图片不存在的情况

某些字段可能为空字符串,下载前应检查:

```python
if url and url.strip():
    download_image(url)
else:
    print("图片URL为空,跳过下载")
```

## 批量处理建议

如果需要批量处理多个店铺数据:

1. **遍历所有数据文件**
2. **为每个店铺创建独立文件夹**
3. **使用店铺ID或名称命名**
4. **记录下载日志**
5. **处理下载失败的重试机制**

### 批量脚本示例

```bash
#!/bin/bash

# 批量处理多个店铺数据文件
for file in *.txt; do
    echo "处理文件: $file"

    # 提取店铺ID作为文件夹名
    store_id=$(basename "$file" .txt)
    output_dir="./店铺图片/$store_id"
    mkdir -p "$output_dir"

    # 这里调用你的提取和下载脚本
    # python extract_images.py "$file" "$output_dir"

    echo "完成: $store_id"
    echo "---"
done

echo "批量处理完成！"
```

## 常见问题

### Q1: 下载的图片无法打开?
**A**: 检查URL是否正确,某些URL可能需要特定的请求头。尝试添加User-Agent:

```bash
curl -H "User-Agent: Mozilla/5.0" -o "image.jpg" "URL"
```

### Q2: 如何判断是否下载了原图?
**A**: 查看文件大小。原图通常在100KB以上,压缩图通常只有几KB到几十KB。

### Q3: 时间戳参数需要保留吗?
**A**: `?t=时间戳` 参数是用于缓存控制的,可以保留也可以删除,不影响图片内容。

### Q4: 有些店铺没有海报图怎么办?
**A**: 不是所有店铺都有活动海报,检查 `operation_source_list` 是否为空数组。

### Q5: 图片URL失效怎么办?
**A**: 美团的CDN图片URL通常长期有效,但如果失效,需要重新获取最新的API数据。

### Q6: Fiddler保存的JSON文件出现乱码怎么办?
**A**: 这是因为响应数据被gzip或deflate压缩了。有以下解决方案:

**方案1: 修改Fiddler脚本** (推荐)
- 在脚本中添加 `oSession.utilDecodeResponse()` 来自动解压
- 参考上面的"自动保存脚本示例"中的完整代码

**方案2: 手动保存解压后的数据**
- 在Fiddler中选中请求
- 右侧 `Inspectors` → `JSON` 视图已自动解压
- 直接复制JSON内容保存

**方案3: 使用Python脚本解压**
```python
import gzip
import json

# 读取压缩的文件
with open('poi_food_20251011_211843.json', 'rb') as f:
    compressed_data = f.read()

# 解压gzip数据
try:
    decompressed_data = gzip.decompress(compressed_data)
    json_text = decompressed_data.decode('utf-8')

    # 验证是否为有效JSON
    data = json.loads(json_text)

    # 保存为可读的JSON文件
    with open('poi_food_20251011_211843_decoded.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print("解压成功!")
except Exception as e:
    print(f"解压失败: {e}")
```

## 总结

提取美团外卖店铺图片的关键步骤:

1. ✅ 从API响应中定位正确的JSON字段
2. ✅ 识别三种主要图片: 头像、店招、海报
3. ✅ **移除URL中的尺寸参数以获取原图**
4. ✅ 使用curl或编程语言下载图片
5. ✅ 验证下载的图片大小和质量

## 参考资源

- 美团外卖API文档: (内部接口,无公开文档)
- 图片CDN域名:
  - `p0.meituan.net`
  - `p1.meituan.net`
  - `img.meituan.net`

---

**文档版本**: 1.0
**更新日期**: 2025-10-11
**作者**: Claude
**适用场景**: 美团外卖店铺数据分析、图片采集
